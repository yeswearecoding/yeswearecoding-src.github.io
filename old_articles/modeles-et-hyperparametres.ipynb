{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello,  \n",
    "  \n",
    "L'un des problèmes lorsque l'on débute en *machine learning* est le choix de l'algorithme (ou modèle) à utiliser. Je viens de tomber sur un [article du blog de Kaggle](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/) où l'auteur partage son approche des différents problèmes à résoudre en *ML*. J'ai noté deux parties qui m'ont éclairé un peu plus sur le choix des modèles et des hyperparamètres à régler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choix d'un modèle\n",
    "\n",
    "Il y a deux grandes familles d'algorithmes : ceux qui permettent de réaliser une prédiction (à l'aide d'une régression) et ceux qui identifie une variable parmi d'autres (la classification). Les modèles les plus courants pour réaliser ses tâches sont donc :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification | Regression\n",
    ":- | -\n",
    "Random Forest | Random Forest\n",
    "GBM | GBM\n",
    "Logistic Regression | Linear Regression\n",
    "Naive Bayes | Ridge\n",
    "Support Vector Machines | Lasso\n",
    "k-Nearest Neighbors | SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choix des hyperparamètres\n",
    "\n",
    "Les hyperparamètres sont ces variables qui permettent d'affiner le fonctionnement d'un modèle de *machine learning*. Jusqu'à présent, j'utilisais des valeurs trouvées dans un livre ou sur des sites internet, sans trop savoir qu'est ce que je pouvais utiliser, jusqu'à quelle valeur je pouvais aller.   \n",
    "Bref, j'y allais à tâtons au \"pif-o-mètre\". Mais l'auteur partage également un tableau récapitulatif de ces différents hyperparamètres et les plages de valeurs les plus prometteuses.  \n",
    "Je le reprend ici :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèle | Hyperparamètre | Plage de données\n",
    " :-------: | :----------------: | :-----------------:\n",
    "**Régression linéaire** | fit_intercept<br>normalize | True / False<br>True / False\n",
    "**Ridge** | alpha<br>fit_intercept<br>normalize | 0.01, 0.1, 1.0, 10, 100<br>True / False<br>True / False\n",
    "**K-neighbors** | N_neighbors<br>p | 2, 4, 8, 16...<br>2,3\n",
    "**SVM** | C<br>gamma<br>class_weight | 0.001, 0.01...10...100...1000<br>Auto ou Random Search<br>Balanced, None\n",
    "**Régression logistique** | Penalty<br>C | L1 ou I2<br>0.001, 0.01...10...100\n",
    "**Naive Bayes** | aucun | aucun\n",
    "**Lasso** | alpha<br>normalize | 0.1, 1.0, 10<br>True / False\n",
    "**Random Forest** | n_estimators<br>max_depth<br>min_samples_split<br>min_samples_leaf<br>max_features | 120, 300, 500, 800, 1200<br>5, 8, 15, 25, 30, None<br>1, 2, 5, 10, 15, 100<br>1, 2, 5, 10<br>Log2, sqrt, None\n",
    "**Xgboost** | eta<br>gamma<br>max_depth<br>min_child_weight<br>subsample<br>colsample_bytree<br>lambda<br>alpha | 0.01, 0.015, 0.025, 0.05, 0.1<br>0.05-0.1, 0.3, 0.5, 0.7, 0.9, 1.0<br>3, 5, 7, 9, 12, 15, 17, 25<br>1, 3, 5, 7<br>0.6, 0.7, 0.8, 0.9, 1.0<br>0.6, 0.7, 0.8, 0.9, 1.0<br>0.01-0.1, 1.0, Random Search<br>0, 0.1, 0.5, 1.0, Random Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'espère que ces petits récapitulatifs vous seront autant utiles qu'à moi ;-)\n",
    "   \n",
    "A bientôt !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
